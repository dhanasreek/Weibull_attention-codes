# -*- coding: utf-8 -*-
"""WADCS  code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jF8Nt8VOgNkFhKF2F8ohO7Z0lzSlpGQp
"""

!pip install -q shap scipy scikit-learn tensorflow

# 1) Imports
import os
import time
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.neighbors import NearestNeighbors
from scipy.stats import weibull_min
import tensorflow as tf
from tensorflow.keras import layers, models, backend as K
import shap

# Reproducibility
SEEDS = [13, 27, 33, 55, 101]
np.random.seed(SEEDS[0])
tf.random.set_seed(SEEDS[0])

# 2) User configuration -- set dataset CSV file paths here
DATASETS = {
    "DNP3": "/content/sample_data/DNP3.csv",
    "CICIDS2017": "/content/sample_data/CICIDS2017.csv",
    "X_IIoTID": "/content/sample_data/X_IIoTID.csv"
}

LABEL_COL = 'target'
WEIBULL_FEATURE = 'duration'

def load_dataset(path, label_col=LABEL_COL):
    df = pd.read_csv(path)

    if label_col not in df.columns:
        candidates = [c for c in df.columns if c.lower() in ('label','class','type')]
        if candidates:
            label_col = candidates[0]
    df = df.dropna(subset=[label_col])

    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if label_col in numeric_cols:
        numeric_cols.remove(label_col)
    df_numeric = df[numeric_cols + [label_col]].copy()
    return df_numeric, numeric_cols, label_col

def make_sequences(X, y, window=20, stride=1):
    # X shape (n_samples, n_features)
    Xs, ys = [], []
    for i in range(0, len(X) - window + 1, stride):
        Xs.append(X[i:i+window])
        ys.append(y[i+window-1])  # label of last time-step in window
    return np.array(Xs), np.array(ys)

def fit_weibull_per_class(df, feature_col, label_col):
    params = {}
    for lbl in df[label_col].unique():
        values = df[df[label_col]==lbl][feature_col].dropna().values
        if len(values) < 10:

            c, loc, scale = 1.0, 0, np.mean(values)+1e-3
        else:
            c, loc, scale = weibull_min.fit(values, floc=0)
        params[lbl] = (c, scale)
    return params

def weibull_survival(x, k, lam):

    return np.exp(- (x / lam) ** k)

def compute_scores_for_rows(df, params, feature_col, label_col):

    scores = []
    for _, row in df.iterrows():
        lbl = row[label_col]
        k, lam = params[lbl]
        val = row[feature_col]
        scores.append(weibull_survival(val, k, lam))
    return np.array(scores)

# Normalize attention scores
def softmax(x, axis=-1):
    e = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return e / np.sum(e, axis=axis, keepdims=True)

# Models

def build_wadcs(input_shape, gru_units=128, dropout=0.3, gamma_attn=0.1):

    inp = layers.Input(shape=input_shape, name="input")
    x = layers.GRU(gru_units, return_sequences=True, name="gru")(inp)
    # learned attention energies e_t
    e = layers.TimeDistributed(layers.Dense(64, activation='tanh'))(x)
    e = layers.TimeDistributed(layers.Dense(1, activation=None))(e)  # (batch, T, 1)
    e = layers.Flatten()(e)  # (batch, T)

    weibull_input = layers.Input(shape=(input_shape[0],), name="weibull")  # (batch, T)

    beta = K.constant(1.0)  # can be tuned; gamma_attn is used in loss
    combined = layers.Lambda(lambda args: args[0] - args[1] * args[2])([e, beta, weibull_input])  # (batch, T)
    alpha = layers.Activation('softmax', name='attention')(combined)  # (batch, T)
    # create context vector
    alpha_exp = layers.Reshape((input_shape[0], 1))(alpha)  # (batch, T, 1)
    context = layers.Multiply()([x, alpha_exp])
    context = layers.Lambda(lambda z: K.sum(z, axis=1))(context)  # (batch, units)
    out = layers.Dropout(dropout)(context)
    out = layers.Dense(64, activation='relu')(out)
    out = layers.Dense(num_classes, activation='softmax')(out)
    model = models.Model(inputs=[inp, weibull_input], outputs=out, name='WADCS')
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def build_gnn_ids(window, features, hidden_dims=(64,32)):

    inp = layers.Input(shape=(window, features))
    x = layers.Flatten()(inp)
    x = layers.Dense(hidden_dims[0], activation='relu')(x)
    x = layers.Dropout(0.2)(x)
    x = layers.Dense(hidden_dims[1], activation='relu')(x)
    out = layers.Dense(num_classes, activation='softmax')(x)
    model = models.Model(inputs=inp, outputs=out, name='GNN_IDS_proxy')
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def build_lightweight(window, features, conv_filters=32, gru_units=32):
    inp = layers.Input(shape=(window, features))
    x = layers.Conv1D(filters=conv_filters, kernel_size=3, padding='same', activation='relu')(inp)
    x = layers.MaxPool1D(pool_size=2)(x)
    x = layers.GRU(gru_units)(x)
    x = layers.Dense(32, activation='relu')(x)
    out = layers.Dense(num_classes, activation='softmax')(x)
    model = models.Model(inputs=inp, outputs=out, name='Lightweight_IDS')
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def build_gru_basic(window, features, units=(128,64)):
    inp = layers.Input(shape=(window, features))
    x = layers.GRU(units[0], return_sequences=True)(inp)
    x = layers.GRU(units[1])(x)
    x = layers.Dense(64, activation='relu')(x)
    out = layers.Dense(num_classes, activation='softmax')(x)
    model = models.Model(inputs=inp, outputs=out, name='GRU_baseline')
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model


def federated_train(model_builder, X_train, y_train, clients=3, rounds=5, local_epochs=50, batch_size=64):

    idx = np.arange(len(X_train))
    np.random.shuffle(idx)
    parts = np.array_split(idx, clients)
    # initialize global weights
    global_model = model_builder()
    global_weights = global_model.get_weights()
    for r in range(rounds):
        new_weights = []
        counts = []
        for p in parts:
            local_model = model_builder()
            local_model.set_weights(global_weights)
            local_X = X_train[p]
            local_y = y_train[p]
            if len(local_X) == 0:
                continue
            local_model.fit(local_X, local_y, epochs=local_epochs, batch_size=batch_size, verbose=0)
            new_weights.append(local_model.get_weights())
            counts.append(len(local_X))
        # weighted average
        # initialize accumulator
        avg = [np.zeros_like(w) for w in new_weights[0]]
        total = sum(counts)
        for w, c in zip(new_weights, counts):
            for i in range(len(w)):
                avg[i] += w[i] * (c / total)
        global_weights = avg
        global_model.set_weights(global_weights)
    return global_model

#  Training / evaluation
def train_and_eval_model(model, inputs_train, y_train, inputs_val, y_val, epochs=100, batch_size=64, patience=20):
    cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)
    history = model.fit(inputs_train, y_train, validation_data=(inputs_val, y_val), epochs=epochs,
                        batch_size=batch_size, callbacks=[cb], verbose=1)

    return model, history

def measure_performance(model, inputs_test, y_test, batch_size=64):

    preds = model.predict(inputs_test, batch_size=batch_size, verbose=0)
    y_pred = np.argmax(preds, axis=1)
    acc = np.mean(y_pred == y_test)

    warm = 1
    n_runs = 10
    times = []
    for r in range(n_runs):
        t0 = time.perf_counter()
        model.predict(inputs_test, batch_size=batch_size, verbose=0)
        t1 = time.perf_counter()
        times.append(t1 - t0)
    avg_time = np.mean(times)  # seconds per full test pass
    total_samples = len(inputs_test)
    latency_ms = (avg_time / total_samples) * 1000  # ms per sample
    throughput = total_samples / avg_time
    return acc, latency_ms, throughput

# 7) Main pipeline for a dataset
def pipeline_for_dataset(path, dataset_name, window=20, test_size=0.15, val_size=0.15):
    print("=== Dataset:", dataset_name, "path:", path)
    df, numeric_cols, label_col = load_dataset(path)
    print("Numeric features found:", len(numeric_cols))
    # label encode
    le = LabelEncoder()
    df[label_col] = le.fit_transform(df[label_col].astype(str))
    global num_classes
    num_classes = len(le.classes_)
    # compute weibull params
    feature_col = WEIBULL_FEATURE if WEIBULL_FEATURE in df.columns else numeric_cols[0]
    print("Weibull feature:", feature_col)
    weibull_params = fit_weibull_per_class(df, feature_col, label_col)
    df['weibull_score'] = compute_scores_for_rows(df, weibull_params, feature_col, label_col)
    # preprocessing: scale numeric features
    scaler = MinMaxScaler()
    X_all = scaler.fit_transform(df[numeric_cols])
    y_all = df[label_col].values
    # create sequences
    X_seq, y_seq = make_sequences(X_all, y_all, window=window, stride=1)
    # align weibull scores to sequence labels
    weibull_arr = df['weibull_score'].values
    weibull_seq = []
    for i in range(0, len(weibull_arr) - window + 1):
        weibull_seq.append(weibull_arr[i:i+window])
    weibull_seq = np.array(weibull_seq)
    # train/val/test split
    X_temp, X_test, y_temp, y_test, w_temp, w_test = train_test_split(
        X_seq, y_seq, weibull_seq, test_size=test_size, random_state=SEEDS[0], stratify=y_seq)
    rel_val = val_size / (1 - test_size)
    X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(
        X_temp, y_temp, w_temp, test_size=rel_val, random_state=SEEDS[0], stratify=y_temp)
    print("Shapes (train,val,test):", X_train.shape, X_val.shape, X_test.shape)
    results = {}

    # -- WADCS
    print("\n>>> Training WADCS")
    wadcs = build_wadcs(input_shape=(window, len(numeric_cols)), gru_units=128)
    # train with [X, weibull] inputs
    wadcs.fit([X_train, w_train[:,:, -1] if False else w_train.mean(axis=1)], y_train,
               validation_data=([X_val, w_val.mean(axis=1)], y_val),
               epochs=120, batch_size=64, callbacks=[tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)], verbose=1)

    wadcs_acc, wadcs_lat, wadcs_thr = measure_performance(wadcs, [X_test, w_test.mean(axis=1)], y_test)
    results['WADCS'] = (wadcs_acc*100, wadcs_lat, wadcs_thr)

    # -- GNN-IDS
    print("\n>>> Training GNN-IDS (kNN aggregation proxy)")
    # Precompute kNN aggregated features per window: take mean of kNN of flattened windows
    def knn_aggregate(X_windows, k=5):
        # flatten windows
        flattened = X_windows.reshape((X_windows.shape[0], -1))
        nbrs = NearestNeighbors(n_neighbors=min(k, len(flattened)-1)).fit(flattened)
        neigh_idx = nbrs.kneighbors(flattened, return_distance=False)
        agg = np.array([flattened[inds].mean(axis=0) for inds in neigh_idx])
        # reshape back to window,features
        return agg.reshape((agg.shape[0], X_windows.shape[1], X_windows.shape[2]))
    X_train_g = knn_aggregate(X_train)
    X_val_g = knn_aggregate(X_val)
    X_test_g = knn_aggregate(X_test)
    gnn = build_gnn_ids(window, len(numeric_cols))
    gnn.fit(X_train_g, y_train, validation_data=(X_val_g, y_val), epochs=100, batch_size=64,
            callbacks=[tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)], verbose=1)
    gnn_acc, gnn_lat, gnn_thr = measure_performance(gnn, X_test_g, y_test)
    results['GNN_IDS'] = (gnn_acc*100, gnn_lat, gnn_thr)

    # -- FL-IDS (simulate FedAvg)
    print("\n>>> Training FL-IDS (FedAvg simulation)")
    def fl_builder():
        return build_gru_basic(window, len(numeric_cols), units=(128,))  # simple local GRU
    fl_model = federated_train(lambda: build_gru_basic(window, len(numeric_cols), units=(128,)),
                               X_train, y_train, clients=3, rounds=10, local_epochs=100, batch_size=64)
    fl_acc, fl_lat, fl_thr = measure_performance(fl_model, X_test, y_test)
    results['FL_IDS'] = (fl_acc*100, fl_lat, fl_thr)

    # -- Lightweight IDS
    print("\n>>> Training Lightweight IDS")
    lw = build_lightweight(window, len(numeric_cols), conv_filters=32, gru_units=32)
    lw.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=64,
           callbacks=[tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)], verbose=1)
    lw_acc, lw_lat, lw_thr = measure_performance(lw, X_test, y_test)
    results['Lightweight_IDS'] = (lw_acc*100, lw_lat, lw_thr)

    # -- XAI-IDS (GRU + SHAP)
    print("\n>>> Training XAI-IDS")
    xai = build_gru_basic(window, len(numeric_cols), units=(128,64))
    xai.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=64,
            callbacks=[tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)], verbose=1)
    xai_acc, xai_lat, xai_thr = measure_performance(xai, X_test, y_test)
    results['XAI_IDS'] = (xai_acc*100, xai_lat, xai_thr)
    # Prepare SHAP explanations for a few test samples (optional)
    try:
        explainer = shap.KernelExplainer(lambda z: xai.predict(z), X_train[:100])
        shap_vals = explainer.shap_values(X_test[:10])
        # save or inspect as needed
    except Exception as e:
        print("SHAP explanation skipped (slow) or failed:", e)

    # summarize
    df_res = pd.DataFrame.from_dict(results, orient='index', columns=['Accuracy(%)', 'Latency(ms/sample)', 'Throughput(s/sec)'])
    df_res['Dataset'] = dataset_name
    print(df_res)
    return df_res

# 8) Run pipeline for each dataset
all_results = []
for name, path in DATASETS.items():
    if not os.path.exists(path):
        print(f"Dataset file {path} not found. Please upload and set correct path.")
        continue
    res = pipeline_for_dataset(path, name, window=20)
    all_results.append(res)

# 9) Save combined results
if all_results:
    final_df = pd.concat(all_results)
    final_df.to_csv("models_comparison_results.csv", index=True)
    print("Saved summary to models_comparison_results.csv")
else:
    print("No datasets processed (check file paths).")

def zero_day_split(X, y, zero_day_ratio=0.10):

    X_train, X_zd, y_train, y_zd = train_test_split(
        X, y, test_size=zero_day_ratio, stratify=y
    )
    X_train, X_test, y_train, y_test = train_test_split(
        X_train, y_train, test_size=0.15, stratify=y_train
    )
    return X_train, X_test, X_zd, y_train, y_test, y_zd

for name, model in models.items():

        print(f"Training: {name} on {ds_name}")

        model.compile(optimizer="adam", loss="binary_crossentropy")

        # Select correct input format
        if name in ["GNN-IDS"]:
            model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=0)
            pred = model.predict(X_zd, verbose=0).flatten()
            latency, throughput = measure_latency_throughput(model, X_zd)
        else:
            model.fit(X_train_seq, y_train, epochs=100, batch_size=64, verbose=0)
            pred = model.predict(X_zd_seq, verbose=0).flatten()
            latency, throughput = measure_latency_throughput(model, X_zd_seq)

        # compute metrics
        acc, rec, f1, roc, fpr = compute_metrics(y_zd, pred)

        results.append([
            ds_name, name, acc*100, rec, f1, roc, fpr*100, latency, throughput
        ])

        gc.collect()

# RESULTS TABLE


df = pd.DataFrame(results, columns=[
    "Dataset", "Model", "Accuracy (%)", "Recall (Zero-Day)", "F1-Score",
    "ROC-AUC", "FPR (%)", "Latency (ms)", "Throughput (samples/sec)"
])

print("\nFINAL RESULTS")
print(df)

df.to_csv("IDS_results.csv", index=False)
print("\nSaved results to IDS_results.csv")